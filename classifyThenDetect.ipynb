{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "from picamera2 import Picamera2\n",
    "from libcamera import Transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from folder Depth-Anything-V2/depth_anything_v2/\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "def init():\n",
    "    global model\n",
    "# small, base, \n",
    "    model_configs = {\n",
    "        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
    "    }\n",
    "\n",
    "    encoder = 'vits' # or 'vits', 'vitb'\n",
    "    dataset = 'hypersim' # 'hypersim' for indoor model, 'vkitti' for outdoor model\n",
    "    max_depth = 20 # 20 for indoor model, 80 for outdoor model\n",
    "\n",
    "    #! removed max depth\n",
    "    model = DepthAnythingV2(**{**model_configs[encoder], 'max_depth': max_depth})\n",
    "    # download models from: \n",
    "    model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_metric_{dataset}_{encoder}.pth', map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateDepth(picam_array):\n",
    "    global original_depth\n",
    "    raw_img = picam_array\n",
    "\n",
    "    start_time = time.time()\n",
    "    original_depth = model.infer_image(raw_img) # HxW depth map in meters in numpy\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Inference time: {end_time - start_time} seconds\")\n",
    "\n",
    "    depth = original_depth.copy()\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    depth = depth.astype(np.uint8)\n",
    "    cmap = matplotlib.colormaps.get_cmap('Spectral')\n",
    "\n",
    "    depth = (cmap(depth)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite('output/metric_depth_map.png', depth)\n",
    "\n",
    "\n",
    "    model_base = original_depth.copy()\n",
    "\n",
    "\n",
    "    # plt.imshow(original_depth)\n",
    "    # plt.colorbar(label='Depth (m)')\n",
    "\n",
    "    return np.array(model_base)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init()\n",
    "# estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n",
    "\n",
    "# test: adding 13.5cm to entire image, vs just adding 13.5cm at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evalSideHelper(index, r):\n",
    "    def get_center(c):\n",
    "        moments = cv2.moments(c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2))\n",
    "        if moments[\"m00\"] != 0:\n",
    "            return moments[\"m10\"] / moments[\"m00\"]\n",
    "        return None\n",
    "\n",
    "    centers = []\n",
    "    for i, c in enumerate(r):\n",
    "        center = get_center(c)\n",
    "        if center is not None:\n",
    "            centers.append((center, i))\n",
    "    centers = sorted(centers)\n",
    "\n",
    "    if len(centers) == 3:\n",
    "        if centers[0][1] == index:\n",
    "            return \"left\"\n",
    "        elif centers[1][1] == index:\n",
    "            return \"center\"\n",
    "        else:\n",
    "            return \"right\"\n",
    "    elif len(centers) == 2:\n",
    "        if centers[0][1] == index:\n",
    "            return \"left\"\n",
    "        else:\n",
    "            return \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def erode_contour(contour, image_shape, kernel_size=5, iterations=1):\n",
    "    \"\"\"\n",
    "    Shrinks a contour by applying morphological erosion.\n",
    "\n",
    "    Parameters:\n",
    "    - contour: np.array, the original contour (Nx1x2)\n",
    "    - image_shape: tuple, (height, width) of the image/mask\n",
    "    - kernel_size: int, size of the erosion kernel (must be odd, e.g., 3, 5, 7)\n",
    "    - iterations: int, number of erosion steps (more iterations = more shrinking)\n",
    "\n",
    "    Returns:\n",
    "    - new_contour: np.array, the shrunken contour\n",
    "    \"\"\"\n",
    "    # Create a blank mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Draw the contour as a filled shape\n",
    "    cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Define an erosion kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    # Apply erosion to shrink the contour\n",
    "    eroded_mask = cv2.erode(mask, kernel, iterations=iterations)\n",
    "\n",
    "    # Extract the new smaller contour\n",
    "    new_contours, _ = cv2.findContours(eroded_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Return the first found contour (if any)\n",
    "    return new_contours[0] if new_contours else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pipe = None\n",
    "\n",
    "# for debug purposes\n",
    "depthImage = None \n",
    "\n",
    "def estimate(picam_array, bottleBox, masks, res):\n",
    "    global depthImage\n",
    "    \n",
    "    print(\"Starting depth estimation. This will take about 20-30 seconds\")\n",
    "    depthImage = calculateDepth(picam_array)\n",
    "    # values are: avarage, maximum, centerpoint, 3x3 avg, 3x3 max\n",
    "    depth_values = {\n",
    "        \"left\": [None, None, None, None, None],\n",
    "        \"center\": [None, None, None, None, None],\n",
    "        \"right\": [None, None, None, None, None]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate detection results \n",
    "    for r in res:\n",
    "\n",
    "        img = np.copy(r.orig_img)\n",
    "        img_name = Path(r.path).stem\n",
    "\n",
    "        # Iterate each object contour \n",
    "        for ci, c in enumerate(r):\n",
    "            label = c.names[c.boxes.cls.tolist().pop()]\n",
    "            print(f\"{ci + 1}. {label}\")\n",
    "            \n",
    "            sideString = \"center\"\n",
    "            if len(r) > 1:\n",
    "                sideString = evalSideHelper(ci, r)\n",
    "\n",
    "\n",
    "\n",
    "            # just a full black mask\n",
    "            b_mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "\n",
    "            # Create contour mask \n",
    "            bigcontour = c.masks.xy[0].astype(np.int32).reshape(-1, 1, 2)\n",
    "            smallerContour = erode_contour(bigcontour, b_mask.shape, kernel_size=5, iterations=4)\n",
    "\n",
    "            # ! warning ! uses smallerContour\n",
    "            _ = cv2.drawContours(b_mask, [smallerContour], -1, (255, 255, 255), cv2.FILLED)\n",
    "            contour = smallerContour\n",
    "\n",
    "            # Calculate the center point of the contour\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            else:\n",
    "                cX, cY = 0, 0\n",
    "            print(f\"Center point of the contour: ({cX}, {cY})\")\n",
    "\n",
    "            # get depth at centerpoint\n",
    "            depthAtBottleCenter = depthImage[cY, cX]\n",
    "\n",
    "            #!!\n",
    "            # Get a 3x3 array of pixels around the centerpoint\n",
    "            # top left\n",
    "            x_start = max(cX - 1, 0)\n",
    "            # bottom right\n",
    "            x_end = min(cX + 2, depthImage.shape[1])\n",
    "            y_start = max(cY - 1, 0)\n",
    "            y_end = min(cY + 2, depthImage.shape[0])\n",
    "            pixel_array = depthImage[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate average and max values of the 3x3 array\n",
    "            avg_3x3 = np.mean(pixel_array)\n",
    "            max_3x3 = np.max(pixel_array)\n",
    "\n",
    "            # Add to depth_values\n",
    "            depth_values[sideString][3] = avg_3x3\n",
    "            depth_values[sideString][4] = max_3x3\n",
    "            #!!\n",
    "\n",
    "\n",
    "            # Choose one:\n",
    "\n",
    "            # OPTION-1: Isolate object with black background\n",
    "            # mask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n",
    "            # isolated = cv2.bitwise_and(mask3ch, img)\n",
    "\n",
    "\n",
    "            # Convert depthImage to 3-channel image\n",
    "            # depthImage_3ch = cv2.cvtColor(depthImage, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            isolated = cv2.bitwise_and(depthImage, depthImage, mask=b_mask)\n",
    "            \n",
    "            # Calculate the average depth value for the masked object\n",
    "            masked_depth = cv2.bitwise_and(depthImage, depthImage, mask=b_mask)\n",
    "            average_depth = cv2.mean(masked_depth, mask=b_mask)[0]\n",
    "\n",
    "\n",
    "            # add to depth_values\n",
    "            depth_values[sideString][0] = average_depth\n",
    "            # max depth of masked_depth\n",
    "            depth_values[sideString][1] = masked_depth.max()\n",
    "            depth_values[sideString][2] = depthAtBottleCenter\n",
    "\n",
    "            print(f\"Average depth of the object: {average_depth}\")\n",
    "\n",
    "            calibratedDistance = 0.15\n",
    "            print(f\"For calibration, adding {calibratedDistance}: {average_depth + calibratedDistance}\")\n",
    "\n",
    "\n",
    "\n",
    "            # Display the isolated object using matplotlib\n",
    "            plt.imshow(isolated)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return depth_values\n",
    "\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from libcamera import Transform\n",
    "import time\n",
    "\n",
    "# from depthest import init, estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel():\n",
    "    bottleFrameDetected, bottleBox, bottleMask, segmentResults = None, None, None, None\n",
    "\n",
    "    finalEstimates = None\n",
    "    print(\"init\")\n",
    "    with Picamera2() as picam2:\n",
    "        # Initialize the Picamera2\n",
    "        # picam2 = Picamera2()\n",
    "\n",
    "        # default\n",
    "        # picam2.preview_configuration.main.size = (1280, 720)\n",
    "        picam2.preview_configuration.main.size = (640, 310)\n",
    "        # picam2.preview_configuration.main.size = (640 /2, 310/2)\n",
    "\n",
    "        picam2.preview_configuration.main.format = \"RGB888\"\n",
    "        picam2.preview_configuration.align()\n",
    "        picam2.preview_configuration.transform=Transform(vflip=1)\n",
    "        picam2.configure(\"preview\")\n",
    "        picam2.start()\n",
    "\n",
    "\n",
    "        # Load the YOLO11 model\n",
    "        # have to run create model first \n",
    "        segmentModel = YOLO(\"yolo11n-seg.pt\")  \n",
    "        model = YOLO(\"yolo11n-cls.pt\")\n",
    "\n",
    "        instantBreak = False\n",
    "\n",
    "\n",
    "        # used to record the time when we processed last frame \n",
    "        prev_frame_time = 0\n",
    "        \n",
    "        # used to record the time at which we processed current frame \n",
    "        new_frame_time = 0\n",
    "        # font which we will be using to display FPS \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "        bottleFrameDetected = None\n",
    "        bottleFrameNew = None\n",
    "\n",
    "        results = None\n",
    "        bottleBox = None\n",
    "\n",
    "        while True:\n",
    "            # time when we finish processing for this frame \n",
    "            # ! do all processing below this, and above the fps calculator\n",
    "            new_frame_time = time.time() \n",
    "\n",
    "            # Capture frame-by-frame\n",
    "            frame = picam2.capture_array()\n",
    "\n",
    "            # Run YOLO11 inference on the frame\n",
    "            results = model(frame)\n",
    "\n",
    "            # print(results)\n",
    "            # Visualize the results on the frame\n",
    "            # annotated_frame = results[0].plot()\n",
    "\n",
    "            new_frame_time = time.time() \n",
    "        \n",
    "            # Calculating the fps \n",
    "            fps = 1/(new_frame_time-prev_frame_time) \n",
    "            prev_frame_time = new_frame_time \n",
    "            fps = int(fps) \n",
    "            fps = str(fps) \n",
    "            # cv2.putText(annotated_frame, fps, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA) \n",
    "            print(fps)\n",
    "            # Check if a bottle has been recognized\n",
    "            for result in results:\n",
    "                # probs = result.probs  # Probs object for classification outputs\n",
    "                # print(probs)\n",
    "                # classification model\n",
    "                if result.probs != None:\n",
    "                    # print(\"Top 5 classes\", result.probs.top5)\n",
    "                    # https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "                    # 440: beer_bottle\n",
    "                    # 898: water bottle\n",
    "                    if 898 in result.probs.top5:\n",
    "                        print(\"Bottle found with class index 898\")\n",
    "                        bottleFrameDetected = frame\n",
    "                        instantBreak = True\n",
    "                        break;\n",
    "\n",
    "\n",
    "                # for detection in result.boxes:\n",
    "                # #     # Assuming detection.cls is an integer index for the class\n",
    "                #     bottleBox = detection\n",
    "                #     if detection.cls == 39:  # correct class index for \"bottle\"\n",
    "                #         print(\"Bottle recognized\", )\n",
    "                #         print(\"Probabilty\", detection.conf)\n",
    "                #         bottleFrameDetected = frame\n",
    "                #         instantBreak = True\n",
    "                #         break\n",
    "\n",
    "\n",
    "            # Display the resulting frame\n",
    "            # cv2.imshow(\"Camera\", annotated_frame)\n",
    "\n",
    "            if instantBreak:\n",
    "                break\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                print(\"\")\n",
    "                break\n",
    "\n",
    "        # Release resources and close windows\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # https://docs.ultralytics.com/modes/predict/#__tabbed_1_1\n",
    "        if bottleFrameDetected is not None:\n",
    "            # Capture a new frame of the bottle\n",
    "            # should not be blurry\n",
    "            bottleFrameNew = picam2.capture_array()\n",
    "            cv2.imwrite(\"output/bottle_new.png\", bottleFrameNew)\n",
    "\n",
    "        # Process results list\n",
    "        # results for bottleFrameDetected\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "            masks = result.masks  # Masks object for segmentation masks outputs\n",
    "            keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "            probs = result.probs  # Probs object for classification outputs\n",
    "            obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "            # result.show()  # display to screen\n",
    "            result.save(filename=\"output/bottle_detected.png\")  # save to disk\n",
    "\n",
    "        # segmnet\n",
    "        segmentResults = segmentModel.predict(source=bottleFrameDetected, classes=39)\n",
    "        bottleMask = None\n",
    "        # Process results list\n",
    "        for result in segmentResults:\n",
    "            result.save(filename=\"output/segmented.jpg\")  # save to disk\n",
    "\n",
    "            if (result.masks.shape[0] > 3) :\n",
    "                print(\"More than three bottles detected. Evaluation helper function can't handle more than three bottles..\")\n",
    "\n",
    "\n",
    "            bottleMask = result.masks[0]  # Masks object for segmentation masks outputs\n",
    "\n",
    "        init()\n",
    "        finalEstimates = estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Stop the camera to free up resources\n",
    "    picam2.stop() \n",
    "    picam2.stop_encoder()\n",
    "\n",
    "    del picam2\n",
    "\n",
    "    return finalEstimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[313:55:21.778603825] [1925475] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:325 \u001b[0mlibcamera v0.3.2+99-1230f78d\n",
      "[313:55:21.809091123] [1927135] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:40 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[313:55:21.811143698] [1927135] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:447 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/imx708@1a to Unicam device /dev/media1 and ISP device /dev/media2\n",
      "[313:55:21.811201957] [1927135] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1120 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n",
      "[313:55:21.821751571] [1925475] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x310-RGB888 (1) 1536x864-SGRBG10_CSI2P\n",
      "[313:55:21.822215956] [1927135] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:622 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/imx708@1a - Selected sensor format: 1536x864-SGRBG10_1X10 - Selected unicam format: 1536x864-pgAA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 water_bottle 0.80, pop_bottle 0.10, beer_bottle 0.03, beer_glass 0.01, vacuum 0.01, 117.3ms\n",
      "Speed: 20.5ms preprocess, 117.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "0\n",
      "Bottle found with class index 898\n",
      "\n",
      "0: 320x640 3 bottles, 747.8ms\n",
      "Speed: 6.1ms preprocess, 747.8ms inference, 11.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Starting depth estimation. This will take about 20-30 seconds\n",
      "Inference time: 25.833521127700806 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mrunModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;66;03m# prettyprintEstimates(result)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 141\u001b[0m, in \u001b[0;36mrunModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m         bottleMask \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmasks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Masks object for segmentation masks outputs\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     init()\n\u001b[0;32m--> 141\u001b[0m     finalEstimates \u001b[38;5;241m=\u001b[39m \u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleFrameDetected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleBox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleMask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentResults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Stop the camera to free up resources\u001b[39;00m\n\u001b[1;32m    147\u001b[0m picam2\u001b[38;5;241m.\u001b[39mstop() \n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(picam_array, bottleBox, masks, res)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m     28\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(r\u001b[38;5;241m.\u001b[39morig_img)\n\u001b[0;32m---> 29\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(r\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mstem\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Iterate each object contour \u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ci, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(r):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def prettyprintEstimates(estimates):\n",
    "  for side, values in estimates.items():\n",
    "    print(f\"Bottle: {side}\")\n",
    "    print(f\"Average depth: {values[0]}\")\n",
    "    print(f\"Max depth: {values[1]}\")\n",
    "    print(f\"Depth at centerpoint: {values[2]}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "# run entireModel 4 times\n",
    "results = []\n",
    "for _ in range(4):\n",
    "  result = runModel()\n",
    "  results.append(result)\n",
    "  # prettyprintEstimates(result)\n",
    "\n",
    "print('finished. printing!!!')\n",
    "print('finished. printing!!!')\n",
    "print('finished. printing!!!')\n",
    "\n",
    "# Pretty print the results\n",
    "for i, result in enumerate(results):\n",
    "  print(f\"Run {i + 1}:\")\n",
    "  prettyprintEstimates(result)\n",
    "\n",
    "\n",
    "  # Define the CSV file path\n",
    "  csv_file_path = 'output/depth_estimates.csv'\n",
    "\n",
    "  # Define the header\n",
    "  header = [\n",
    "    'left_average', 'left_max', 'left_centerpoint', \"left_3x3_avg\", \"left_3x3_max\",\n",
    "    'center_average', 'center_max', 'center_centerpoint', \"center_3x3_avg\", \"center_3x3_max\",\n",
    "    'right_average', 'right_max', 'right_centerpoint', \"right_3x3_avg\", \"right_3x3_max\"\n",
    "  ]\n",
    "\n",
    "  # Write the data to the CSV file\n",
    "  with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for result in results:\n",
    "      row = [\n",
    "        result['left'][0], result['left'][1], result['left'][2], result['left'][3], result['left'][4],\n",
    "        result['center'][0], result['center'][1], result['center'][2],  result['center'][3], result['center'][4],\n",
    "        result['right'][0], result['right'][1], result['right'][2], result['right'][3], result['right'][4]\n",
    "\n",
    "      ]\n",
    "      writer.writerow(row)\n",
    "\n",
    "  print(f\"Data exported to {csv_file_path}\")\n",
    "# calculate error\n",
    "# calculate what is standard error\n",
    "# depth vs error\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
