{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "from picamera2 import Picamera2\n",
    "from libcamera import Transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from folder Depth-Anything-V2/depth_anything_v2/\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "def init():\n",
    "    global model\n",
    "# small, base, \n",
    "    model_configs = {\n",
    "        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
    "    }\n",
    "\n",
    "    encoder = 'vits' # or 'vits', 'vitb'\n",
    "    dataset = 'hypersim' # 'hypersim' for indoor model, 'vkitti' for outdoor model\n",
    "    max_depth = 20 # 20 for indoor model, 80 for outdoor model\n",
    "\n",
    "    #! removed max depth\n",
    "    model = DepthAnythingV2(**{**model_configs[encoder],})\n",
    "    # download models from: \n",
    "    model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_metric_{dataset}_{encoder}.pth', map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateDepth(picam_array):\n",
    "    global original_depth\n",
    "    raw_img = picam_array\n",
    "\n",
    "    start_time = time.time()\n",
    "    original_depth = model.infer_image(raw_img) # HxW depth map in meters in numpy\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Inference time: {end_time - start_time} seconds\")\n",
    "\n",
    "    depth = original_depth.copy()\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    depth = depth.astype(np.uint8)\n",
    "    cmap = matplotlib.colormaps.get_cmap('Spectral')\n",
    "\n",
    "    depth = (cmap(depth)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite('depth_map.png', depth)\n",
    "\n",
    "\n",
    "    model_base = original_depth.copy()\n",
    "    plt.imshow(original_depth)\n",
    "    plt.colorbar(label='Depth (m)')\n",
    "    return np.array(model_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "pipe = None\n",
    "\n",
    "# def init():\n",
    "#     global pipe\n",
    "#     pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "#     print(\"depth est. init\")\n",
    "\n",
    "# def calculateDepth(picam_array):\n",
    "#     pi_image = Image.fromarray(picam_array)\n",
    "#     # inference\n",
    "#     depth = pipe(pi_image)[\"depth\"]\n",
    "#     depth.save(\"output/depth_estimation.png\")\n",
    "\n",
    "#     # calculate the depth of the bottle\n",
    "#     # calculate average depth of pixels inside mask\n",
    "#     depth_array = np.array(depth)\n",
    "#     return depth_array\n",
    "\n",
    "def estimate(picam_array, bottleBox, masks, res):\n",
    "    # Iterate detection results \n",
    "    for r in res:\n",
    "        img = np.copy(r.orig_img)\n",
    "        img_name = Path(r.path).stem\n",
    "\n",
    "        # Iterate each object contour \n",
    "        for ci, c in enumerate(r):\n",
    "            label = c.names[c.boxes.cls.tolist().pop()]\n",
    "            print(label)\n",
    "\n",
    "            # just a full black mask\n",
    "            b_mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "            # Create contour mask \n",
    "            contour = c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2)\n",
    "            _ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "            # Choose one:\n",
    "\n",
    "            # OPTION-1: Isolate object with black background\n",
    "            mask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n",
    "            # isolated = cv2.bitwise_and(mask3ch, img)\n",
    "\n",
    "            depthImage = calculateDepth(picam_array)\n",
    "            # print(\"Shape of mask3ch\" ,mask3ch)\n",
    "            # print(\"Shape of depth array\", depthImage)\n",
    "\n",
    "            # Convert depthImage to 3-channel image\n",
    "            depthImage_3ch = cv2.cvtColor(depthImage, cv2.COLOR_GRAY2BGR)\n",
    "            isolated = cv2.bitwise_and(mask3ch, depthImage_3ch)\n",
    "\n",
    "            # Calculate the average depth value for the masked object\n",
    "            masked_depth = cv2.bitwise_and(depthImage, depthImage, mask=b_mask)\n",
    "            average_depth = cv2.mean(masked_depth, mask=b_mask)[0]\n",
    "            print(f\"Average depth of the object: {average_depth}\")\n",
    "\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Display the isolated object using matplotlib\n",
    "            plt.imshow(cv2.cvtColor(isolated, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate2(picam_array, bottleBox, masks, results):\n",
    "    pi_image = Image.fromarray(picam_array)\n",
    "    # inference\n",
    "    depth = pipe(pi_image)[\"depth\"]\n",
    "    depth.save(\"output/depth_estimation.png\")\n",
    "\n",
    "    # calculate the depth of the bottle\n",
    "    # calculate average depth of pixels inside mask\n",
    "    depth_array = np.array(depth)\n",
    "\n",
    "    b_mask = np.zeros(depth_array.shape[:2], np.uint8)\n",
    "\n",
    "\n",
    "    contour = c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2)\n",
    "    _ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "    average_depth = np.mean(contour)\n",
    "\n",
    "    print(f\"Average depth of the bottle: {average_depth}\")\n",
    "\n",
    "    return average_depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from libcamera import Transform\n",
    "import time\n",
    "\n",
    "# from depthest import init, estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:14:46.304095827] [4711] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:325 \u001b[0mlibcamera v0.3.2+99-1230f78d\n",
      "[0:14:46.332737869] [7857] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:40 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[0:14:46.334676327] [7857] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:447 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/imx708@1a to Unicam device /dev/media1 and ISP device /dev/media0\n",
      "[0:14:46.334733049] [7857] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1120 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n",
      "[0:14:46.344450266] [4711] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x310-RGB888 (1) 1536x864-SGRBG10_CSI2P\n",
      "[0:14:46.344885543] [7857] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:622 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/imx708@1a - Selected sensor format: 1536x864-SGRBG10_1X10 - Selected unicam format: 1536x864-pgAA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 water_bottle 0.83, pop_bottle 0.13, nipple 0.01, perfume 0.01, beer_bottle 0.01, 146.6ms\n",
      "Speed: 16.3ms preprocess, 146.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "0\n",
      "Bottle found with class index 898\n",
      "\n",
      "0: 320x640 2 bottles, 706.5ms\n",
      "Speed: 4.7ms preprocess, 706.5ms inference, 10.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Error! More than one bottle detected. Using the first one.\n",
      "Error! More than one bottle detected. Using the first one.\n",
      "Error! More than one bottle detected. Using the first one.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DepthAnythingV2.__init__() got an unexpected keyword argument 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 137\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError! More than one bottle detected. Using the first one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m         bottleMask \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmasks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Masks object for segmentation masks outputs\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Stop the camera to free up resources\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypersim\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 'hypersim' for indoor model, 'vkitti' for outdoor model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;66;03m# 20 for indoor model, 80 for outdoor model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDepthAnythingV2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# download models from: \u001b[39;00m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/depth_anything_v2_metric_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: DepthAnythingV2.__init__() got an unexpected keyword argument 'max_depth'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"init\")\n",
    "with Picamera2() as picam2:\n",
    "    # Initialize the Picamera2\n",
    "    # picam2 = Picamera2()\n",
    "\n",
    "    # default\n",
    "    # picam2.preview_configuration.main.size = (1280, 720)\n",
    "    picam2.preview_configuration.main.size = (640, 310)\n",
    "    # picam2.preview_configuration.main.size = (640 /2, 310/2)\n",
    "\n",
    "    picam2.preview_configuration.main.format = \"RGB888\"\n",
    "    picam2.preview_configuration.align()\n",
    "    picam2.preview_configuration.transform=Transform(vflip=1)\n",
    "    picam2.configure(\"preview\")\n",
    "    picam2.start()\n",
    "\n",
    "\n",
    "    # Load the YOLO11 model\n",
    "    # have to run create model first \n",
    "    segmentModel = YOLO(\"yolo11n-seg.pt\")  \n",
    "    model = YOLO(\"yolo11n-cls.pt\")\n",
    "\n",
    "    instantBreak = False\n",
    "\n",
    "\n",
    "    # used to record the time when we processed last frame \n",
    "    prev_frame_time = 0\n",
    "    \n",
    "    # used to record the time at which we processed current frame \n",
    "    new_frame_time = 0\n",
    "    # font which we will be using to display FPS \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "    bottleFrameDetected = None\n",
    "    bottleFrameNew = None\n",
    "\n",
    "    results = None\n",
    "    bottleBox = None\n",
    "\n",
    "    while True:\n",
    "        # time when we finish processing for this frame \n",
    "        # ! do all processing below this, and above the fps calculator\n",
    "        new_frame_time = time.time() \n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        frame = picam2.capture_array()\n",
    "\n",
    "        # Run YOLO11 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # print(results)\n",
    "        # Visualize the results on the frame\n",
    "        # annotated_frame = results[0].plot()\n",
    "\n",
    "        new_frame_time = time.time() \n",
    "    \n",
    "        # Calculating the fps \n",
    "        fps = 1/(new_frame_time-prev_frame_time) \n",
    "        prev_frame_time = new_frame_time \n",
    "        fps = int(fps) \n",
    "        fps = str(fps) \n",
    "        # cv2.putText(annotated_frame, fps, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA) \n",
    "        print(fps)\n",
    "        # Check if a bottle has been recognized\n",
    "        for result in results:\n",
    "            # probs = result.probs  # Probs object for classification outputs\n",
    "            # print(probs)\n",
    "            # classification model\n",
    "            if result.probs != None:\n",
    "                # print(\"Top 5 classes\", result.probs.top5)\n",
    "                # https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "                # 440: beer_bottle\n",
    "                # 898: water bottle\n",
    "                if 898 in result.probs.top5:\n",
    "                    print(\"Bottle found with class index 898\")\n",
    "                    bottleFrameDetected = frame\n",
    "                    instantBreak = True\n",
    "                    break;\n",
    "\n",
    "\n",
    "            # for detection in result.boxes:\n",
    "            # #     # Assuming detection.cls is an integer index for the class\n",
    "            #     bottleBox = detection\n",
    "            #     if detection.cls == 39:  # correct class index for \"bottle\"\n",
    "            #         print(\"Bottle recognized\", )\n",
    "            #         print(\"Probabilty\", detection.conf)\n",
    "            #         bottleFrameDetected = frame\n",
    "            #         instantBreak = True\n",
    "            #         break\n",
    "\n",
    "\n",
    "        # Display the resulting frame\n",
    "        # cv2.imshow(\"Camera\", annotated_frame)\n",
    "\n",
    "        if instantBreak:\n",
    "            break\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "    # Release resources and close windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # https://docs.ultralytics.com/modes/predict/#__tabbed_1_1\n",
    "    if bottleFrameDetected is not None:\n",
    "        # Capture a new frame of the bottle\n",
    "        # should not be blurry\n",
    "        bottleFrameNew = picam2.capture_array()\n",
    "        cv2.imwrite(\"output/bottle_new.png\", bottleFrameNew)\n",
    "\n",
    "    # Process results list\n",
    "    # results for bottleFrameDetected\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "        masks = result.masks  # Masks object for segmentation masks outputs\n",
    "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "        probs = result.probs  # Probs object for classification outputs\n",
    "        obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "        # result.show()  # display to screen\n",
    "        result.save(filename=\"output/bottle_detected.png\")  # save to disk\n",
    "\n",
    "    # segmnet\n",
    "    segmentResults = segmentModel.predict(source=bottleFrameDetected, classes=39)\n",
    "    bottleMask = None\n",
    "    # Process results list\n",
    "    for result in segmentResults:\n",
    "        result.save(filename=\"output/segmented.jpg\")  # save to disk\n",
    "\n",
    "        if (result.masks.shape[0] > 1) :\n",
    "            print(\"Error! More than one bottle detected. Using the first one.\")\n",
    "            print(\"Error! More than one bottle detected. Using the first one.\")\n",
    "            print(\"Error! More than one bottle detected. Using the first one.\")\n",
    "\n",
    "        bottleMask = result.masks[0]  # Masks object for segmentation masks outputs\n",
    "\n",
    "    init()\n",
    "    estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stop the camera to free up resources\n",
    "picam2.stop() \n",
    "picam2.stop_encoder()\n",
    "\n",
    "del picam2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle\n",
      "Inference time: 25.292192935943604 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4711/3652699315.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
      "/tmp/ipykernel_4711/3652699315.py:35: RuntimeWarning: invalid value encountered in cast\n",
      "  depth = depth.astype(np.uint8)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/core/src/arithm.cpp:212: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array' in function 'binary_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m init()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleFrameDetected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleBox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleMask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentResults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(picam_array, bottleBox, masks, res)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(\"Shape of mask3ch\" ,mask3ch)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(\"Shape of depth array\", depthImage)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Convert depthImage to 3-channel image\u001b[39;00m\n\u001b[1;32m     52\u001b[0m depthImage_3ch \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(depthImage, cv2\u001b[38;5;241m.\u001b[39mCOLOR_GRAY2BGR)\n\u001b[0;32m---> 53\u001b[0m isolated \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitwise_and\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask3ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepthImage_3ch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Calculate the average depth value for the masked object\u001b[39;00m\n\u001b[1;32m     56\u001b[0m masked_depth \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_and(depthImage, depthImage, mask\u001b[38;5;241m=\u001b[39mb_mask)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/core/src/arithm.cpp:212: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array' in function 'binary_op'\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
