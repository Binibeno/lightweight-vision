{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "from picamera2 import Picamera2\n",
    "from libcamera import Transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from folder Depth-Anything-V2/depth_anything_v2/\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "def init():\n",
    "    global model\n",
    "# small, base, \n",
    "    model_configs = {\n",
    "        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
    "    }\n",
    "\n",
    "    encoder = 'vits' # or 'vits', 'vitb'\n",
    "    dataset = 'hypersim' # 'hypersim' for indoor model, 'vkitti' for outdoor model\n",
    "    max_depth = 20 # 20 for indoor model, 80 for outdoor model\n",
    "\n",
    "    #! removed max depth\n",
    "    model = DepthAnythingV2(**{**model_configs[encoder], 'max_depth': max_depth})\n",
    "    # download models from: \n",
    "    model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_metric_{dataset}_{encoder}.pth', map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateDepth(picam_array):\n",
    "    global original_depth\n",
    "    raw_img = picam_array\n",
    "\n",
    "    start_time = time.time()\n",
    "    original_depth = model.infer_image(raw_img) # HxW depth map in meters in numpy\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Inference time: {end_time - start_time} seconds\")\n",
    "\n",
    "    depth = original_depth.copy()\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    depth = depth.astype(np.uint8)\n",
    "    cmap = matplotlib.colormaps.get_cmap('Spectral')\n",
    "\n",
    "    depth = (cmap(depth)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite('output/metric_depth_map.png', depth)\n",
    "\n",
    "\n",
    "    model_base = original_depth.copy()\n",
    "\n",
    "\n",
    "    # plt.imshow(original_depth)\n",
    "    # plt.colorbar(label='Depth (m)')\n",
    "\n",
    "    return np.array(model_base)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init()\n",
    "# estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n",
    "\n",
    "# test: adding 13.5cm to entire image, vs just adding 13.5cm at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evalSideHelper(index, r):\n",
    "    def get_center(c):\n",
    "        moments = cv2.moments(c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2))\n",
    "        if moments[\"m00\"] != 0:\n",
    "            return moments[\"m10\"] / moments[\"m00\"]\n",
    "        return None\n",
    "\n",
    "    centers = []\n",
    "    for i, c in enumerate(r):\n",
    "        center = get_center(c)\n",
    "        if center is not None:\n",
    "            centers.append((center, i))\n",
    "    centers = sorted(centers)\n",
    "\n",
    "    if len(centers) == 3:\n",
    "        if centers[0][1] == index:\n",
    "            return \"left\"\n",
    "        elif centers[1][1] == index:\n",
    "            return \"center\"\n",
    "        else:\n",
    "            return \"right\"\n",
    "    elif len(centers) == 2:\n",
    "        if centers[0][1] == index:\n",
    "            return \"left\"\n",
    "        else:\n",
    "            return \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pipe = None\n",
    "\n",
    "# for debug purposes\n",
    "depthImage = None \n",
    "\n",
    "def estimate(picam_array, bottleBox, masks, res):\n",
    "    global depthImage\n",
    "    \n",
    "    print(\"Starting depth estimation. This will take about 20-30 seconds\")\n",
    "    depthImage = calculateDepth(picam_array)\n",
    "    # values are: avarage, maximum, centerpoint\n",
    "    depth_values = {\n",
    "        \"left\": [None, None, None],\n",
    "        \"center\": [None, None, None],\n",
    "        \"right\": [None, None, None]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate detection results \n",
    "    for r in res:\n",
    "\n",
    "        img = np.copy(r.orig_img)\n",
    "        img_name = Path(r.path).stem\n",
    "\n",
    "        # Iterate each object contour \n",
    "        for ci, c in enumerate(r):\n",
    "            label = c.names[c.boxes.cls.tolist().pop()]\n",
    "            print(f\"{ci + 1}. {label}\")\n",
    "            \n",
    "            sideString = \"center\"\n",
    "            if len(r) > 1:\n",
    "                sideString = evalSideHelper(ci, r)\n",
    "\n",
    "\n",
    "\n",
    "            # just a full black mask\n",
    "            b_mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "\n",
    "            # Create contour mask \n",
    "            # contour = c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2)\n",
    "            _ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "            # Calculate the center point of the contour\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            else:\n",
    "                cX, cY = 0, 0\n",
    "            print(f\"Center point of the contour: ({cX}, {cY})\")\n",
    "\n",
    "            # get depth at centerpoint\n",
    "            depthAtBottleCenter = depthImage[cY, cX]\n",
    "\n",
    "\n",
    "            # Choose one:\n",
    "\n",
    "            # OPTION-1: Isolate object with black background\n",
    "            # mask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n",
    "            # isolated = cv2.bitwise_and(mask3ch, img)\n",
    "\n",
    "\n",
    "            # Convert depthImage to 3-channel image\n",
    "            # depthImage_3ch = cv2.cvtColor(depthImage, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            isolated = cv2.bitwise_and(depthImage, depthImage, mask=b_mask)\n",
    "            \n",
    "            # Calculate the average depth value for the masked object\n",
    "            masked_depth = cv2.bitwise_and(depthImage, depthImage, mask=b_mask)\n",
    "            average_depth = cv2.mean(masked_depth, mask=b_mask)[0]\n",
    "\n",
    "\n",
    "            # add to depth_values\n",
    "            depth_values[sideString][0] = average_depth\n",
    "            # max depth of masked_depth\n",
    "            depth_values[sideString][1] = masked_depth.max()\n",
    "            depth_values[sideString][2] = depthAtBottleCenter\n",
    "\n",
    "            print(f\"Average depth of the object: {average_depth}\")\n",
    "\n",
    "            calibratedDistance = 0.15\n",
    "            print(f\"For calibration, adding {calibratedDistance}: {average_depth + calibratedDistance}\")\n",
    "\n",
    "\n",
    "\n",
    "            # Display the isolated object using matplotlib\n",
    "            plt.imshow(isolated)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return depth_values\n",
    "\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from picamera2 import Picamera2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from libcamera import Transform\n",
    "import time\n",
    "\n",
    "# from depthest import init, estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel():\n",
    "    bottleFrameDetected, bottleBox, bottleMask, segmentResults = None, None, None, None\n",
    "\n",
    "    finalEstimates = None\n",
    "    print(\"init\")\n",
    "    with Picamera2() as picam2:\n",
    "        # Initialize the Picamera2\n",
    "        # picam2 = Picamera2()\n",
    "\n",
    "        # default\n",
    "        # picam2.preview_configuration.main.size = (1280, 720)\n",
    "        picam2.preview_configuration.main.size = (640, 310)\n",
    "        # picam2.preview_configuration.main.size = (640 /2, 310/2)\n",
    "\n",
    "        picam2.preview_configuration.main.format = \"RGB888\"\n",
    "        picam2.preview_configuration.align()\n",
    "        picam2.preview_configuration.transform=Transform(vflip=1)\n",
    "        picam2.configure(\"preview\")\n",
    "        picam2.start()\n",
    "\n",
    "\n",
    "        # Load the YOLO11 model\n",
    "        # have to run create model first \n",
    "        segmentModel = YOLO(\"yolo11n-seg.pt\")  \n",
    "        model = YOLO(\"yolo11n-cls.pt\")\n",
    "\n",
    "        instantBreak = False\n",
    "\n",
    "\n",
    "        # used to record the time when we processed last frame \n",
    "        prev_frame_time = 0\n",
    "        \n",
    "        # used to record the time at which we processed current frame \n",
    "        new_frame_time = 0\n",
    "        # font which we will be using to display FPS \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "        bottleFrameDetected = None\n",
    "        bottleFrameNew = None\n",
    "\n",
    "        results = None\n",
    "        bottleBox = None\n",
    "\n",
    "        while True:\n",
    "            # time when we finish processing for this frame \n",
    "            # ! do all processing below this, and above the fps calculator\n",
    "            new_frame_time = time.time() \n",
    "\n",
    "            # Capture frame-by-frame\n",
    "            frame = picam2.capture_array()\n",
    "\n",
    "            # Run YOLO11 inference on the frame\n",
    "            results = model(frame)\n",
    "\n",
    "            # print(results)\n",
    "            # Visualize the results on the frame\n",
    "            # annotated_frame = results[0].plot()\n",
    "\n",
    "            new_frame_time = time.time() \n",
    "        \n",
    "            # Calculating the fps \n",
    "            fps = 1/(new_frame_time-prev_frame_time) \n",
    "            prev_frame_time = new_frame_time \n",
    "            fps = int(fps) \n",
    "            fps = str(fps) \n",
    "            # cv2.putText(annotated_frame, fps, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA) \n",
    "            print(fps)\n",
    "            # Check if a bottle has been recognized\n",
    "            for result in results:\n",
    "                # probs = result.probs  # Probs object for classification outputs\n",
    "                # print(probs)\n",
    "                # classification model\n",
    "                if result.probs != None:\n",
    "                    # print(\"Top 5 classes\", result.probs.top5)\n",
    "                    # https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "                    # 440: beer_bottle\n",
    "                    # 898: water bottle\n",
    "                    if 898 in result.probs.top5:\n",
    "                        print(\"Bottle found with class index 898\")\n",
    "                        bottleFrameDetected = frame\n",
    "                        instantBreak = True\n",
    "                        break;\n",
    "\n",
    "\n",
    "                # for detection in result.boxes:\n",
    "                # #     # Assuming detection.cls is an integer index for the class\n",
    "                #     bottleBox = detection\n",
    "                #     if detection.cls == 39:  # correct class index for \"bottle\"\n",
    "                #         print(\"Bottle recognized\", )\n",
    "                #         print(\"Probabilty\", detection.conf)\n",
    "                #         bottleFrameDetected = frame\n",
    "                #         instantBreak = True\n",
    "                #         break\n",
    "\n",
    "\n",
    "            # Display the resulting frame\n",
    "            # cv2.imshow(\"Camera\", annotated_frame)\n",
    "\n",
    "            if instantBreak:\n",
    "                break\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                print(\"\")\n",
    "                break\n",
    "\n",
    "        # Release resources and close windows\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # https://docs.ultralytics.com/modes/predict/#__tabbed_1_1\n",
    "        if bottleFrameDetected is not None:\n",
    "            # Capture a new frame of the bottle\n",
    "            # should not be blurry\n",
    "            bottleFrameNew = picam2.capture_array()\n",
    "            cv2.imwrite(\"output/bottle_new.png\", bottleFrameNew)\n",
    "\n",
    "        # Process results list\n",
    "        # results for bottleFrameDetected\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "            masks = result.masks  # Masks object for segmentation masks outputs\n",
    "            keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "            probs = result.probs  # Probs object for classification outputs\n",
    "            obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "            # result.show()  # display to screen\n",
    "            result.save(filename=\"output/bottle_detected.png\")  # save to disk\n",
    "\n",
    "        # segmnet\n",
    "        segmentResults = segmentModel.predict(source=bottleFrameDetected, classes=39)\n",
    "        bottleMask = None\n",
    "        # Process results list\n",
    "        for result in segmentResults:\n",
    "            result.save(filename=\"output/segmented.jpg\")  # save to disk\n",
    "\n",
    "            if (result.masks.shape[0] > 3) :\n",
    "                print(\"More than three bottles detected. Evaluation helper function can't handle more than three bottles..\")\n",
    "\n",
    "\n",
    "            bottleMask = result.masks[0]  # Masks object for segmentation masks outputs\n",
    "\n",
    "        init()\n",
    "        finalEstimates = estimate(bottleFrameDetected, bottleBox, bottleMask, segmentResults)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Stop the camera to free up resources\n",
    "    picam2.stop() \n",
    "    picam2.stop_encoder()\n",
    "\n",
    "    del picam2\n",
    "\n",
    "    return finalEstimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:41:29.563156283] [9365] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:325 \u001b[0mlibcamera v0.3.2+99-1230f78d\n",
      "[0:41:29.624311235] [12410] \u001b[1;33m WARN \u001b[1;37mRPiSdn \u001b[1;34msdn.cpp:40 \u001b[0mUsing legacy SDN tuning - please consider moving SDN inside rpi.denoise\n",
      "[0:41:29.626334685] [12410] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:447 \u001b[0mRegistered camera /base/soc/i2c0mux/i2c@1/imx708@1a to Unicam device /dev/media4 and ISP device /dev/media1\n",
      "[0:41:29.626391500] [12410] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpipeline_base.cpp:1120 \u001b[0mUsing configuration file '/usr/share/libcamera/pipeline/rpi/vc4/rpi_apps.yaml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0:41:29.669412404] [9365] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x310-RGB888 (1) 1536x864-SGRBG10_CSI2P\n",
      "[0:41:29.670084473] [12410] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mvc4.cpp:622 \u001b[0mSensor: /base/soc/i2c0mux/i2c@1/imx708@1a - Selected sensor format: 1536x864-SGRBG10_1X10 - Selected unicam format: 1536x864-pgAA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 water_bottle 0.94, pop_bottle 0.02, water_jug 0.01, nipple 0.01, cocktail_shaker 0.01, 102.9ms\n",
      "Speed: 18.9ms preprocess, 102.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
      "0\n",
      "Bottle found with class index 898\n",
      "\n",
      "0: 320x640 3 bottles, 701.7ms\n",
      "Speed: 8.5ms preprocess, 701.7ms inference, 12.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Starting depth estimation. This will take about 20-30 seconds\n",
      "Inference time: 27.25689196586609 seconds\n",
      "1. bottle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mrunModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m   results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     15\u001b[0m   prettyprintEstimates(result)\n",
      "Cell \u001b[0;32mIn[33], line 141\u001b[0m, in \u001b[0;36mrunModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m         bottleMask \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmasks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Masks object for segmentation masks outputs\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     init()\n\u001b[0;32m--> 141\u001b[0m     finalEstimates \u001b[38;5;241m=\u001b[39m \u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleFrameDetected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleBox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49mbottleMask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentResults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Stop the camera to free up resources\u001b[39;00m\n\u001b[1;32m    147\u001b[0m picam2\u001b[38;5;241m.\u001b[39mstop() \n",
      "Cell \u001b[0;32mIn[31], line 42\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(picam_array, bottleBox, masks, res)\u001b[0m\n\u001b[1;32m     40\u001b[0m sideString \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     sideString \u001b[38;5;241m=\u001b[39m \u001b[43mevalSideHelper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create contour mask \u001b[39;00m\n\u001b[1;32m     45\u001b[0m contour \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mmasks\u001b[38;5;241m.\u001b[39mxy\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m, in \u001b[0;36mevalSideHelper\u001b[0;34m(index, r)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm00\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m centers \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m centers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(centers)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(centers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm00\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m centers \u001b[38;5;241m=\u001b[39m [(\u001b[43mget_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m, i) \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(r) \u001b[38;5;28;01mif\u001b[39;00m get_center(c) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      9\u001b[0m centers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(centers)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(centers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m, in \u001b[0;36mevalSideHelper.<locals>.get_center\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_center\u001b[39m(c):\n\u001b[0;32m----> 3\u001b[0m     moments \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmoments(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm00\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m moments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm00\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "def prettyprintEstimates(estimates):\n",
    "  for side, values in estimates.items():\n",
    "    print(f\"Bottle: {side}\")\n",
    "    print(f\"Average depth: {values[0]}\")\n",
    "    print(f\"Max depth: {values[1]}\")\n",
    "    print(f\"Depth at centerpoint: {values[2]}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "# run entireModel 4 times\n",
    "results = []\n",
    "for _ in range(4):\n",
    "  result = runModel()\n",
    "  results.append(result)\n",
    "  prettyprintEstimates(result)\n",
    "\n",
    "\n",
    "# Pretty print the results\n",
    "for i, result in enumerate(results):\n",
    "  print(f\"Run {i + 1}:\")\n",
    "  prettyprintEstimates(result)\n",
    "\n",
    "\n",
    "# calculate error\n",
    "# calculate what is standard error\n",
    "# depth vs error\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
